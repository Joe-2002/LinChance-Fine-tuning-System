# ç”¨äºå­˜å‚¨ä¸åŒè¯­è¨€çš„æ–‡æœ¬
language_texts = {
    "English": {
        "sidebar_title": "Fine-tuning Parameters",
        "info_btn_text": "â„¹ï¸ Info",
        "dataset_directory": "Dataset Directory:",
        "dataset_info": "Directory where your dataset is located.",
        "dataset_name": "Dataset Name:",
        "dataset_name_info": "Name of your dataset.",
        "model_name_or_path": "Model Name or Path:",
        "model_name_or_path_info": "Name or path of the pre-trained model.",
        "output_directory": "Output Directory:",
        "output_dir_info": "Directory where the fine-tuned model will be saved.",
        "batch_size_per_device": "Batch Size per Device:",
        
        "batch_size_info": """
        Number of samples processed in one iteration on each GPU. \n
        ğŸ“ˆ Larger batch size can lead to faster training but may require more GPU memory. \n
        ğŸ“‰ Smaller batch size can reduce GPU memory usage but may slow down training.
        """,
        
        "accumulation_steps": "Gradient Accumulation Steps:",
        "accumulation_steps_info": """
        Number of steps before backpropagation and optimization. \n
        ğŸ“ˆ Increasing accumulation steps can accumulate gradients over more steps and may help with memory constraints. \n
        ğŸ“‰ Decreasing accumulation steps can reduce memory usage but may slow down training.
        """,
        
        "learning_rate": "Learning Rate:",
        "learning_rate_info": """
        Rate at which the model's parameters are updated. \n
        ğŸ“ˆ Increasing learning rate can lead to faster convergence but may cause instability. \n
        ğŸ“‰ Decreasing learning rate can make the training more stable but may slow down convergence.
        """,
        
        "num_train_epochs": "Number of Training Epochs:",
        "epochs_info": """
        Number of times the entire dataset is passed through the model. \n
        ğŸ“ˆ Increasing epochs can improve model performance but may increase training time. \n
        ğŸ“‰ Decreasing epochs may result in underfitting.
        """,
        
        "lora_rank": "LoRA Rank:",
        "lora_rank_info": """
        Rank parameter for LoRA method. \n
        ğŸ“ˆ Increasing rank can capture more complex patterns but may increase computation. \n
        ğŸ“‰ Decreasing rank may simplify the model but may lose information.
        """,
        
        "lora_alpha": "LoRA Alpha:",
        "lora_alpha_info": """
        Alpha parameter for LoRA method. \n
        ğŸ“ˆ Increasing alpha can give more weight to local context but may overfit. \n
        ğŸ“‰ Decreasing alpha may make the model more globally focused but may underfit.
        """,
        
        "eval_steps": "Evaluation Steps:",
        "eval_steps_info": """
        Number of steps between model evaluation. \n
        ğŸ“ˆ Increasing evaluation steps can reduce evaluation frequency but may save time. \n
        ğŸ“‰ Decreasing evaluation steps may provide more frequent feedback but may increase time.
        """,
        
        "logging_steps": "Logging Steps:",
        "logging_steps_info": """
        Number of steps between logs. \n
        ğŸ“ˆ Increasing logging steps can reduce log frequency but may save space. \n
        ğŸ“‰ Decreasing logging steps may provide more detailed logs but may use more space.
        """,
        
        "save_steps": "Save Steps:",
        "save_steps_info": """
        Number of steps between model saving. \n
        ğŸ“ˆ Increasing save steps can reduce model saving frequency but may save space. \n
        ğŸ“‰ Decreasing save steps may provide more frequent model checkpoints but may use more space.
        """,
        
        "enable_fp16": "Enable FP16",
        "fp16_info": """
        Enable or disable FP16 training. \n
        ğŸ“ˆ Enabling FP16 can reduce memory usage but may affect numerical stability. \n
        ğŸ“‰ Disabling FP16 may require more GPU memory but may improve numerical stability.
        """,
    },
    "ä¸­æ–‡": {
        "sidebar_title": "å¾®è°ƒå‚æ•°",
        "info_btn_text": "â„¹ï¸ ä¿¡æ¯",
        "dataset_directory": "æ•°æ®é›†ç›®å½•ï¼š",
        "dataset_info": "æ•°æ®é›†æ‰€åœ¨çš„ç›®å½•ã€‚",
        "dataset_name": "æ•°æ®é›†åç§°ï¼š",
        "dataset_name_info": "æ‚¨çš„æ•°æ®é›†çš„åç§°ã€‚",
        "model_name_or_path": "æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼š",
        "model_name_or_path_info": "é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„ã€‚",
        "output_directory": "è¾“å‡ºç›®å½•ï¼š",
        "output_dir_info": "ä¿å­˜å¾®è°ƒæ¨¡å‹çš„ç›®å½•ã€‚",
        "batch_size_per_device": "æ¯ä¸ªè®¾å¤‡çš„æ‰¹é‡å¤§å°ï¼š",
        "batch_size_info": """
        åœ¨æ¯ä¸ªGPUä¸Šä¸€æ¬¡è¿­ä»£ä¸­å¤„ç†çš„æ ·æœ¬æ•°é‡ã€‚ \n
        ğŸ“ˆ è¾ƒå¤§çš„æ‰¹é‡å¤§å°å¯èƒ½å¯¼è‡´æ›´å¿«çš„è®­ç»ƒï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šçš„GPUå†…å­˜ã€‚ \n
        ğŸ“‰ è¾ƒå°çš„æ‰¹é‡å¤§å°å¯ä»¥å‡å°‘GPUå†…å­˜ä½¿ç”¨ï¼Œä½†å¯èƒ½å‡æ…¢è®­ç»ƒé€Ÿåº¦ã€‚
        """,
        "accumulation_steps": "æ¢¯åº¦ç´¯ç§¯æ¬¡æ•°ï¼š",
        "accumulation_steps_info": """
        åå‘ä¼ æ’­å’Œä¼˜åŒ–ä¹‹å‰çš„æ­¥æ•°ã€‚ \n
        ğŸ“ˆ å¢åŠ ç´¯ç§¯æ­¥æ•°å¯ä»¥åœ¨æ›´å¤šæ­¥éª¤ä¸Šç´¯ç§¯æ¢¯åº¦ï¼Œå¹¶å¯èƒ½æœ‰åŠ©äºå†…å­˜çº¦æŸã€‚ \n
        ğŸ“‰ å‡å°‘ç´¯ç§¯æ­¥æ•°å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†å¯èƒ½å‡æ…¢è®­ç»ƒé€Ÿåº¦ã€‚
        """,
        "learning_rate": "å­¦ä¹ ç‡ï¼š",
        "learning_rate_info": """
        æ¨¡å‹å‚æ•°æ›´æ–°çš„é€Ÿç‡ã€‚ \n
        ğŸ“ˆ å¢åŠ å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´æ›´å¿«çš„æ”¶æ•›ï¼Œä½†å¯èƒ½å¯¼è‡´ä¸ç¨³å®šæ€§ã€‚ \n
        ğŸ“‰ å‡å°å­¦ä¹ ç‡å¯ä»¥ä½¿è®­ç»ƒæ›´ç¨³å®šï¼Œä½†å¯èƒ½å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚
        """,
        "num_train_epochs": "è®­ç»ƒè½®æ•°ï¼š",
        "epochs_info": """
        æ•´ä¸ªæ•°æ®é›†é€šè¿‡æ¨¡å‹çš„æ¬¡æ•°ã€‚ \n
        ğŸ“ˆ å¢åŠ è½®æ•°å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å¯èƒ½å¢åŠ è®­ç»ƒæ—¶é—´ã€‚ \n
        ğŸ“‰ å‡å°‘è½®æ•°å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆã€‚
        """,
        "lora_rank": "LoRAç­‰çº§ï¼š",
        "lora_rank_info": """
        LoRAæ–¹æ³•çš„ç­‰çº§å‚æ•°ã€‚ \n
        ğŸ“ˆ å¢åŠ ç­‰çº§å¯ä»¥æ•æ‰æ›´å¤æ‚çš„æ¨¡å¼ï¼Œä½†å¯èƒ½å¢åŠ è®¡ç®—ã€‚ \n
        ğŸ“‰ å‡å°ç­‰çº§å¯èƒ½ç®€åŒ–æ¨¡å‹ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¿¡æ¯ã€‚
        """,
        "lora_alpha": "LoRA Alphaï¼š",
        "lora_alpha_info": """
        LoRAæ–¹æ³•çš„Alphaå‚æ•°ã€‚ \n
        ğŸ“ˆ å¢åŠ Alphaå¯ä»¥æ›´å¤šåœ°æƒè¡¡å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆã€‚ \n
        ğŸ“‰ å‡å°Alphaå¯èƒ½ä½¿æ¨¡å‹æ›´åŠ å…¨å±€ï¼Œä½†å¯èƒ½æ¬ æ‹Ÿåˆã€‚
        """,
        "eval_steps": "è¯„ä¼°æ­¥æ•°ï¼š",
        "eval_steps_info": """
        æ¨¡å‹è¯„ä¼°ä¹‹é—´çš„æ­¥æ•°ã€‚ \n
        ğŸ“ˆ å¢åŠ è¯„ä¼°æ­¥æ•°å¯ä»¥å‡å°‘è¯„ä¼°é¢‘ç‡ï¼Œä½†å¯èƒ½èŠ‚çœæ—¶é—´ã€‚ \n
        ğŸ“‰ å‡å°‘è¯„ä¼°æ­¥æ•°å¯èƒ½æä¾›æ›´é¢‘ç¹çš„åé¦ˆï¼Œä½†å¯èƒ½å¢åŠ æ—¶é—´ã€‚
        """,
        "logging_steps": "æ—¥å¿—è®°å½•æ­¥æ•°ï¼š",
        "logging_steps_info": """
        è®°å½•ä¹‹é—´çš„æ­¥æ•°ã€‚ \n
        ğŸ“ˆ å¢åŠ æ—¥å¿—è®°å½•æ­¥æ•°å¯ä»¥å‡å°‘æ—¥å¿—é¢‘ç‡ï¼Œä½†å¯èƒ½èŠ‚çœç©ºé—´ã€‚ \n
        ğŸ“‰ å‡å°‘æ—¥å¿—è®°å½•æ­¥æ•°å¯èƒ½æä¾›æ›´è¯¦ç»†çš„æ—¥å¿—ï¼Œä½†å¯èƒ½ä½¿ç”¨æ›´å¤šçš„ç©ºé—´ã€‚
        """,
        "save_steps": "ä¿å­˜æ­¥æ•°ï¼š",
        "save_steps_info": """
        æ¨¡å‹ä¿å­˜ä¹‹é—´çš„æ­¥æ•°ã€‚ \n
        ğŸ“ˆ å¢åŠ ä¿å­˜æ­¥æ•°å¯ä»¥å‡å°‘æ¨¡å‹ä¿å­˜é¢‘ç‡ï¼Œä½†å¯èƒ½èŠ‚çœç©ºé—´ã€‚ \n
        ğŸ“‰ å‡å°‘ä¿å­˜æ­¥æ•°å¯èƒ½æä¾›æ›´é¢‘ç¹çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä½†å¯èƒ½ä½¿ç”¨æ›´å¤šçš„ç©ºé—´ã€‚
        """,
        "enable_fp16": "å¯ç”¨FP16",
        "fp16_info": """
        å¯ç”¨æˆ–ç¦ç”¨FP16è®­ç»ƒã€‚ \n
        ğŸ“ˆ å¯ç”¨FP16å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†å¯èƒ½å½±å“æ•°å€¼ç¨³å®šæ€§ã€‚ \n
        ğŸ“‰ ç¦ç”¨FP16å¯èƒ½éœ€è¦æ›´å¤šçš„GPUå†…å­˜ï¼Œä½†å¯èƒ½æé«˜æ•°å€¼ç¨³å®šæ€§ã€‚
        """,
    },
}