import io
import json
import os
import time
import numpy as np
import streamlit as st
import pandas as pd
from io import StringIO
import subprocess
import torch
import torchvision.transforms as transforms
from PIL import Image
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer
from peft import TaskType, get_peft_model, LoraConfig
from tqdm import tqdm
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import torch
from datetime import datetime
import base64
import zipfile
import os
import shutil

# import openai
import streamlit as st
from streamlit_chat import message
from transformers import AutoModel, AutoTokenizer
from datetime import datetime


model=None
selected_dataset =None
options = None
script_path=None
model_name = None
train_scripts = None
model_path = None
global on 
global model_output_dir
lora_target = None 
model_save_path = None # æ¨¡å‹ä¿å­˜è·¯å¾„
merge_scripts = None # åˆå¹¶è„šæœ¬è·¯å¾„

#############
st.sidebar.markdown('<span style="color: #ff6347; font-size: 1.5em; font-weight: bold;">Model</span>', unsafe_allow_html=True)

st.title('Model Fine-tuning System')

# æ¨¡å‹é€‰æ‹©
model_list =  ("ChatGLM3-6B-Chat", "Mistral-7B-Chat", "Llama2-7B-Chat", "Baichuan2-7B-Chat")

options = st.sidebar.selectbox(
    "Select the model you want to fine-tune",
    model_list,
    # index=None,
)

if options == "ChatGLM3-6B-Chat":
    st.header(':red[ChatGLM3-6B-Chat]')
    st.image("/root/LinChance-Fine-tuning-System/images/chatglm.png", width=600,)
    model = "/root/LinChance-Fine-tuning-System/download/download_ChatGLM3-6B-Chat.py"
    model_name = "ChatGLM3-6B-Chat"
    train_scripts = "/root/LinChance-Fine-tuning-System/train_scripts/train_chatglm3.sh"
    model_path = '/root/autodl-tmp/models/ZhipuAI/chatglm3-6b'
    model_output_dir = "/root/LinChance-Fine-tuning-System/output_models/output_chatglm3"
    lora_target = "query_key_value"
    # train_scripts = "/root/LLaMA-Factory/train.sh"
if options == "Mistral-7B-Chat":
    st.header(':red[Mistral-7B-Chat]')
    st.image("/root/LinChance-Fine-tuning-System/images/mistral.png", width=600,)
    model = "/root/LinChance-Fine-tuning-System/download/download_Mistral-7B-Chat.py"
    model_name = "Mistral-7B-Chat"
    train_scripts = "/root/LLaMA-Factory/Mistral_ans"
    model_path = '/root/autodl-tmp/models/TabbyML/Mistral-7B'
    model_output_dir = "/root/LinChance-Fine-tuning-System/output_models/output_mistral"
if options == "Llama2-7B-Chat":
    st.header(':red[Llama2-7B-Chat]')
    st.image("/root/LinChance-Fine-tuning-System/images/llama2.png", width=600,)
    model = "/root/LinChance-Fine-tuning-System/download/download_Llama2-7B-Chat.py"
    model_name = "Llama2-7B-Chat"
    train_scripts = "/root/LLaMA-Factory/Llama2_ans.sh"
    model_path = '/root/autodl-tmp/models/Llama2-7B-Chat'
    model_output_dir = "/root/LinChance-Fine-tuning-System/output_models/output_llama2"
if options == "Baichuan2-7B-Chat":
    st.header(':red[Baichuan2-7B-Chat]')
    st.image("/root/LinChance-Fine-tuning-System/images/baichuan.png", width=600,)
    model = "/root/LinChance-Fine-tuning-System/download/download_Baichuan2-7B-Chat.py"
    model_name = "Baichuan2-7B-Chat"
    train_scripts = "/root/LinChance-Fine-tuning-System/train_scripts/train_baichuan2.sh"
    model_path = '/root/autodl-tmp/models/baichuan-inc/Baichuan2-7B-Chat'
    model_output_dir = "/root/LinChance-Fine-tuning-System/output_models/output_baichuan2"
    lora_target = "W_pack"
    merge_scripts = "/root/LinChance-Fine-tuning-System/merge/merge_scripts/baichuan_export.sh"
########


# æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²ç»å­˜åœ¨æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶å¤¹
models_to_check = ["chatglm3-6b", "Mistral-7B", "Llama-2-7b", "Baichuan2-7B-Chat"]
model_exists = any(os.path.exists(os.path.join("models", model_folder)) for model_folder in models_to_check)

# å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œåˆ™æ˜¾ç¤ºä¸‹è½½æŒ‰é’®
if not model_exists:
    st.title('Model download')
    # æ·»åŠ å†…å®¹
    st.write("Stick here to start Model download")

    # æ·»åŠ æ“ä½œæŒ‰é’®
    if st.button("Download Model", type="primary"):
        # æ‰§è¡Œä¸‹è½½å‘½ä»¤
        st.text(model)
        script_path = model

        # åˆ›å»ºä¸€ä¸ªStringIOå¯¹è±¡ï¼Œç”¨äºæ•è·ä¸‹è½½è¾“å‡º
        download_output = StringIO()

        # è®¾ç½®è¿›åº¦æ¡
        progress_bar = st.progress(0)

        # ä½¿ç”¨subprocess.Popenå¯åŠ¨ä¸‹è½½è„šæœ¬
        process = subprocess.Popen(f"python {script_path}", stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)

        # è·å–æˆ–åˆ›å»ºsession_state.download_info_placeholder
        if "download_info_placeholder" not in st.session_state:
            st.session_state.download_info_placeholder = st.empty()

        # æ˜¾ç¤ºä¸‹è½½è¾“å‡ºçš„å‰ä¸‰è¡Œ
        for i in range(3):
            line = process.stderr.readline()
            st.session_state.download_info_placeholder.text(line.strip())

        with tqdm(total=None, file=download_output, leave=False, disable=True, dynamic_ncols=True) as t:
            while True:
                # å®æ—¶è¯»å–stderrè¾“å‡º
                line = process.stderr.readline()
                if not line:
                    break

                # æŸ¥æ‰¾è¿›åº¦ä¿¡æ¯
                if "Progress:" in line:
                    progress_str = line.split("Progress:")[1].strip()
                    progress = float(progress_str.strip("%")) / 100.0
                    progress_bar.progress(progress)

                # æ›´æ–°tqdmè¾“å‡º
                t.write(line)

                # æ˜¾ç¤ºä¸‹è½½è¾“å‡º
                st.session_state.download_info_placeholder.text(line.strip())

                # ä»…æ˜¾ç¤ºå‰ä¸‰è¡Œ
                if t.n > 2:
                    break

        # ç­‰å¾…ä¸‹è½½å®Œæˆ
        process.wait()

        # æ¸…ç©ºä¸‹è½½ä¿¡æ¯
        st.session_state.download_info_placeholder.empty()

        # æ˜¾ç¤ºæ•´ä¸ªä¸‹è½½è¾“å‡º
        st.code(download_output.getvalue())

        # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
        st.success("Download Complete")

        # è®¾ç½®æ˜¾ç¤ºçª—å£ä½ç½®å’Œå¤§å°
        st.markdown(
            """
            <style>
                .css-1l02zg8 {
                    position: fixed;
                    top: 50%;
                    left: 50%;
                    transform: translate(-50%, -50%);
                    width: 800px;  /* è®¾ç½®çª—å£å®½åº¦ */
                    height: 600px; /* è®¾ç½®çª—å£é«˜åº¦ */
                }
            </style>
            """,
            unsafe_allow_html=True
        )

        
        
        
# ä¾§è¾¹æ æ ‡é¢˜ï¼Œçªå‡ºæ˜¾ç¤ºã€åŠ ç²—å¹¶åŠ ä¸Šé¢œè‰²
st.sidebar.markdown('<span style="color: #ff6347; font-size: 1.5em; font-weight: bold;">Dataset</span>', unsafe_allow_html=True)

# ä¸Šä¼ å‡½æ•°
def upload_and_display_data():
    data_dir = "/root/LLaMA-Factory/data"
    uploaded_file = st.sidebar.file_uploader("Select your dataset and upload it")

    # Automatically name the uploaded dataset as "lima.json"
    file_name = "lima.json"

    if uploaded_file is not None:
        save_file_path = os.path.join(data_dir, file_name)

        # Save the uploaded file as "lima.json"
        with open(save_file_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        st.sidebar.success(f"Success: {save_file_path}")

def select_and_display_dataset():
    data_dir = "/root/LinChance-Fine-tuning-System/LLaMA-Factory/data"
    file_list = [file for file in os.listdir(data_dir) if file.endswith(".json")]

    if not file_list:
        st.warning("No JSON files found in the data directory.")
    else:
        selected_dataset = st.sidebar.selectbox(
            "Choose your dataset",
            file_list,
            index=0  # Set the default index to 0
        )

        if selected_dataset:
            st.title("Dataset")
            st.caption(f'Selected Dataset: :green[{selected_dataset}]')
            selected_file_path = os.path.join(data_dir, selected_dataset)

            with open(selected_file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)

            first_five_data = data[:5]

            expander = st.expander("View first five data entries", expanded=False)
            with expander:
                for entry in first_five_data:
                    st.code(entry)

upload_and_display_data()

select_and_display_dataset()
                    
# ä¾§è¾¹æ æ ‡é¢˜
st.sidebar.markdown('<span style="color: #ff6347; font-size: 1.5em; font-weight: bold;">Fine-tuning Parameters</span>', unsafe_allow_html=True)

# ä¾§è¾¹æ 


def show_info(info_text):
    st.info(info_text)

with st.sidebar.expander("Fine-tuning Parameters", expanded=False):
    # æ•°æ®é›†é€‰æ‹©
    dataset_dir = st.text_input("Dataset Directory:", "/root/LinChance-Fine-tuning-System/LLaMA-Factory/data")
    if st.button("â„¹ï¸ Info", key="dataset_info"):
        show_info("Directory where your dataset is located.")

    dataset_name = st.text_input("Dataset Name:", "lima")
    if st.button("â„¹ï¸ Info", key="dataset_name_info"):
        show_info("Name of your dataset.")

    # æ¨¡å‹ç›¸å…³å‚æ•°
    model_name_or_path = st.text_input("Model Name or Path:", model_path)
    if st.button("â„¹ï¸ Info", key="model_name_or_path_info"):
        show_info("Name or path of the pre-trained model.")

    output_dir = st.text_input("Output Directory:", model_output_dir)
    if st.button("â„¹ï¸ Info", key="output_dir_info"):
        show_info("Directory where the fine-tuned model will be saved.")

    # è®­ç»ƒå‚æ•°
    per_device_train_batch_size = st.slider("Batch Size per Device:", 1, 8, 2)
    if st.button("â„¹ï¸ Info", key="batch_size_info"):
        show_info("""
        Number of samples processed in one iteration on each GPU. \n
        ğŸ“ˆ Larger batch size can lead to faster training but may require more GPU memory. \n
        ğŸ“‰ Smaller batch size can reduce GPU memory usage but may slow down training.
        """)

    gradient_accumulation_steps = st.slider("Gradient Accumulation Steps:", 1, 10, 2)
    if st.button("â„¹ï¸ Info", key="accumulation_steps_info"):
        show_info("""
        Number of steps before backpropagation and optimization. \n
        ğŸ“ˆ Increasing accumulation steps can accumulate gradients over more steps and may help with memory constraints. \n
        ğŸ“‰ Decreasing accumulation steps can reduce memory usage but may slow down training.
        """)

    learning_rate = st.number_input("Learning Rate:", value=5e-5)
    if st.button("â„¹ï¸ Info", key="learning_rate_info"):
        show_info("""
        Rate at which the model's parameters are updated. \n
        ğŸ“ˆ Increasing learning rate can lead to faster convergence but may cause instability. \n
        ğŸ“‰ Decreasing learning rate can make the training more stable but may slow down convergence.
        """)

    num_train_epochs = st.slider("Number of Training Epochs:", 1, 10, 3)
    if st.button("â„¹ï¸ Info", key="epochs_info"):
        show_info("""
        Number of times the entire dataset is passed through the model. \n
        ğŸ“ˆ Increasing epochs can improve model performance but may increase training time. \n
        ğŸ“‰ Decreasing epochs may result in underfitting.
        """)

    # lora å‚æ•°
    lora_rank = st.slider("LoRA Rank:", 1, 20, 10)
    if st.button("â„¹ï¸ Info", key="lora_rank_info"):
        show_info("""
        Rank parameter for LoRA method. \n
        ğŸ“ˆ Increasing rank can capture more complex patterns but may increase computation. \n
        ğŸ“‰ Decreasing rank may simplify the model but may lose information.
        """)

    lora_alpha = st.slider("LoRA Alpha:", 1, 50, 20)
    if st.button("â„¹ï¸ Info", key="lora_alpha_info"):
        show_info("""Alpha parameter for LoRA method. \n
        ğŸ“ˆ Increasing alpha can give more weight to local context but may overfit. \n
        ğŸ“‰ Decreasing alpha may make the model more globally focused but may underfit.
        """)

    # å…¶ä»–å‚æ•°
    eval_steps = st.number_input("Evaluation Steps:", value=500)
    if st.button("â„¹ï¸ Info", key="eval_steps_info"):
        show_info("""
        Number of steps between model evaluation. \n
        ğŸ“ˆ Increasing evaluation steps can reduce evaluation frequency but may save time. \n
        ğŸ“‰ Decreasing evaluation steps may provide more frequent feedback but may increase time.
        """)

    logging_steps = st.number_input("Logging Steps:", value=50)
    if st.button("â„¹ï¸ Info", key="logging_steps_info"):
        show_info("""
        Number of steps between logs. \n
        ğŸ“ˆ Increasing logging steps can reduce log frequency but may save space. \n
        ğŸ“‰ Decreasing logging steps may provide more detailed logs but may use more space.
        """)

    save_steps = st.number_input("Save Steps:", value=500)
    if st.button("â„¹ï¸ Info", key="save_steps_info"):
        show_info("""
        Number of steps between model saving. \n
        ğŸ“ˆ Increasing save steps can reduce model saving frequency but may save space. \n
        ğŸ“‰ Decreasing save steps may provide more frequent model checkpoints but may use more space.
        """)

    # æ˜¯å¦å¯ç”¨ FP16
    fp16 = st.checkbox("Enable FP16", value=True)
    if st.button("â„¹ï¸ Info", key="fp16_info"):
        show_info("""
        Enable or disable FP16 training. \n
        ğŸ“ˆ Enabling FP16 can reduce memory usage but may affect numerical stability. \n
        ğŸ“‰ Disabling FP16 may require more GPU memory but may improve numerical stability.
        """)

st.title("Fine-tuning-scripts")
st.caption(f'Selected Train_scripts: :green[{model_name}]')
expander = st.expander("View first five data entries", expanded=False)

# è®­ç»ƒæŒ‰é’®
if st.sidebar.button("save"):
    current_time = datetime.now().strftime("%Y%m%d%H%M%S")
    
    # æ„å»ºä¿å­˜è·¯å¾„ï¼ŒåŒ…å«æ¨¡å‹åç§°ã€è®­ç»ƒåæ ‡è¯†ç¬¦å’Œå½“å‰æ—¶é—´æˆ³
    # model_save_path = f'{model_output_dir}/{current_time}'
    model_save_path = model_output_dir
    # æ„å»ºå‘½ä»¤
    command = f"""
CUDA_VISIBLE_DEVICES=0 python /root/LinChance-Fine-tuning-System/LLaMA-Factory/src/train_bash.py \\
    --stage sft \\
    --model_name_or_path {model_name_or_path} \\
    --do_train \\
    --dataset_dir {dataset_dir} \\
    --dataset {dataset_name} \\
    --template chatglm3 \\
    --finetuning_type lora \\
    --lora_target {lora_target} \\
    --overwrite_cache \\
    --per_device_train_batch_size {per_device_train_batch_size} \\
    --gradient_accumulation_steps {gradient_accumulation_steps} \\
    --eval_steps {eval_steps} \\
    --lr_scheduler_type cosine \\
    --logging_steps {logging_steps} \\
    --save_steps {save_steps} \\
    --overwrite_output_dir \\
    --output_dir {model_save_path} \\
    --learning_rate {learning_rate} \\
    --num_train_epochs {num_train_epochs} \\
    --plot_loss \\
    --lora_rank {lora_rank} \\
    --lora_alpha {lora_alpha} \\
    --fp16
"""
    with open("/root/LinChance-Fine-tuning-System/train_scripts/train_chatglm3.sh", "w") as file:
        file.write(command)

    # åœ¨ä¿å­˜åæ›´æ–° expander çš„å†…å®¹
    with open(train_scripts, 'r', encoding='utf-8') as script_file:
        script_content = script_file.read()
        # æ›´æ–° expander çš„å†…å®¹
        expander.code(script_content, language="sh")
    # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
    st.success("å‚æ•°ä¿å­˜æˆåŠŸï¼")

# Button to start fine-tuning
if st.button("Start Fine-tuning", type="primary"):
    # Set up progress bar
    progress_bar = st.progress(0)
    
    # Area to display command line output
    console_output = st.empty()
    
    # Print script path
    st.text(f"Running script: {train_scripts}")
    
    # Create StringIO object to capture tqdm output
    tqdm_output = StringIO()

    # Use subprocess.Popen to start the shell script
    process = subprocess.Popen(f"sh {train_scripts}", stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)

    # Wrap stderr stream with tqdm
    with tqdm(total=None, file=tqdm_output, leave=False, disable=True, dynamic_ncols=True) as t:
        while True:
            # Read real-time stderr output
            line = process.stderr.readline()
            if not line:
                break

            # Search for progress information
            if "Progress:" in line:
                progress_str = line.split("Progress:")[1].strip()
                progress = float(progress_str.strip("%")) / 100.0
                progress_bar.progress(progress)

            # Update tqdm output
            t.write(line)

            # Display command line output
            console_output.text(line.strip())

    # Wait for the command to complete
    process.wait()

    # Display the entire command line output
    console_output.code(tqdm_output.getvalue())
    torch.cuda.empty_cache()
    # Display success message
    st.success("Model training process completed successfully!")
    

# åˆå¹¶æ¨¡å‹æƒé‡
st.title('Fine-tuned Merge')
# æ·»åŠ å†…å®¹
st.write("Merge lora to fine-tune model weights")

# è¿è¡Œåˆå¹¶å‡½æ•°
if st.button("Merge", type="primary"):
    # Set up progress bar
    progress_bar = st.progress(0)
    
    # Area to display command line output
    console_output = st.empty()

    # Print script path
    st.text(f"Running script: {merge_scripts}")
    
    # Create StringIO object to capture tqdm output
    tqdm_output = StringIO()

    # Use subprocess.Popen to start the shell script
    process = subprocess.Popen(f"sh {train_scripts}", stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, universal_newlines=True)

    # Wrap stderr stream with tqdm
    with tqdm(total=None, file=tqdm_output, leave=False, disable=True, dynamic_ncols=True) as t:
        while True:
            # Read real-time stderr output
            line = process.stderr.readline()
            if not line:
                break

            # Search for progress information
            if "Progress:" in line:
                progress_str = line.split("Progress:")[1].strip()
                progress = float(progress_str.strip("%")) / 100.0
                progress_bar.progress(progress)

            # Update tqdm output
            t.write(line)

            # Display command line output
            console_output.text(line.strip())

    # Wait for the command to complete
    process.wait()

    # Display the entire command line output
    console_output.code(tqdm_output.getvalue())
    torch.cuda.empty_cache()
    # Display success message
    st.success("Model merge process completed successfully!")   


    
# å®šä¹‰è·å–å‹ç¼©æ–‡ä»¶å†…å®¹çš„å‡½æ•°
def get_zip_file_content(file_path):
    with open(file_path, "rb") as file:
        file_content = file.read()
    return file_content

# æ·»åŠ ä¸€ä¸ªæŒ‰é’®ï¼Œå½“ç‚¹å‡»æ—¶è§¦å‘ä¸‹è½½
st.title('Fine-tuned Model Download')
if st.button("Packing", type="primary"):
    # åœ¨è¿™é‡Œæ·»åŠ ä¸‹è½½æ¨¡å‹çš„é€»è¾‘
    # model_output_dir = "/path/to/your/model/output/directory"

    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨å‹ç¼©æ–‡ä»¶
    temp_dir = "/root/LinChance-Fine-tuning-System/temp"
    os.makedirs(temp_dir, exist_ok=True)

    # è®¾ç½®å‹ç¼©æ–‡ä»¶è·¯å¾„
    zip_file_path = os.path.join(temp_dir, "fine_tuned_model.zip")

    # ä½¿ç”¨ shutil.make_archive åˆ›å»ºå‹ç¼©æ–‡ä»¶
    shutil.make_archive(zip_file_path[:-4], 'zip', model_output_dir)

    # è·å–å‹ç¼©æ–‡ä»¶å†…å®¹
    zip_file_content = get_zip_file_content(zip_file_path)

    # æä¾›ä¸‹è½½æŒ‰é’®
    st.download_button(
        label="Download",
        type="primary",
        data=zip_file_content,
        file_name="fine_tuned_model.zip",
        mime="application/zip"
    )



st.title("Trained Model Chat")
def chat_with_mistral():
    
    return 0
def chat_with_llama2():
    return 0
def chat_with_baichuan2():
    return 0
on = st.toggle("chat with trained model")
if on:
    if model_name == "ChatGLM3-6B-Chat":
        @st.cache_resource
        def get_model_chatglm3():
            MODEL_PATH = os.environ.get('MODEL_PATH', model_path)
            TOKENIZER_PATH = os.environ.get("TOKENIZER_PATH", MODEL_PATH)
            tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)
            model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map="auto").eval()
            return tokenizer, model

        def chat_with_chatglm3():
            tokenizer, model = get_model_chatglm3()

            if "history" not in st.session_state:
                st.session_state.history = []
            if "past_key_values" not in st.session_state:
                st.session_state.past_key_values = None

            max_length = st.sidebar.slider("max_length", 0, 32768, 8192, step=1)
            top_p = st.sidebar.slider("top_p", 0.0, 1.0, 0.8, step=0.01)
            temperature = st.sidebar.slider("temperature", 0.0, 1.0, 0.6, step=0.01)

            buttonClean = st.sidebar.button("æ¸…ç†ä¼šè¯å†å²", key="clean")
            if buttonClean:
                st.session_state.history = []
                st.session_state.past_key_values = None
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                st.rerun()

            for i, message in enumerate(st.session_state.history):
                if message["role"] == "user":
                    with st.chat_message(name="user", avatar="user"):
                        st.markdown(message["content"])
                else:
                    with st.chat_message(name="assistant", avatar="assistant"):
                        st.markdown(message["content"])

            with st.chat_message(name="user", avatar="user"):
                input_placeholder = st.empty()
            with st.chat_message(name="assistant", avatar="assistant"):
                message_placeholder = st.empty()

            prompt_text = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")  # Use text_input instead of chat_input for caching issues
            if prompt_text:
                input_placeholder.markdown(prompt_text)
                history = st.session_state.history
                past_key_values = st.session_state.past_key_values
                for response, history, past_key_values in model.stream_chat(
                        tokenizer,
                        prompt_text,
                        history,
                        past_key_values=past_key_values,
                        max_length=max_length,
                        top_p=top_p,
                        temperature=temperature,
                        return_past_key_values=True,
                ):
                    message_placeholder.markdown(response)
                st.session_state.history = history
                st.session_state.past_key_values = past_key_values
                
        chat_with_chatglm3()

    if model_name == "Mistral-7B-Chat":
        chat_with_mistral()
    if model_name == "Llama2-7B-Chat":
        chat_with_llama2()
    if model_name == "Baichuan2-7B-Chat":
        chat_with_baichuan2()
