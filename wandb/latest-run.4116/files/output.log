
01/29/2024 00:38:31 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.










































  4%|█████▌                                                                                                                     | 49/1089 [01:26<29:11,  1.68s/it]













































  9%|███████████▏                                                                                                               | 99/1089 [02:56<29:25,  1.78s/it]











































 14%|████████████████▋                                                                                                         | 149/1089 [04:22<26:12,  1.67s/it]













































 18%|██████████████████████▍                                                                                                   | 200/1089 [05:53<27:10,  1.83s/it][INFO|trainer.py:2926] 2024-01-29 00:44:24,916 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-200
{'loss': 0.3805, 'learning_rate': 4.614752397304091e-05, 'epoch': 0.55}
[INFO|tokenization_utils_base.py:2433] 2024-01-29 00:44:24,963 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-29 00:44:24,964 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-200/special_tokens_map.json











































 23%|████████████████████████████                                                                                              | 250/1089 [07:23<23:28,  1.68s/it]












































 27%|█████████████████████████████████▍                                                                                        | 299/1089 [08:51<23:31,  1.79s/it]













































 32%|███████████████████████████████████████▏                                                                                  | 350/1089 [10:22<21:01,  1.71s/it]












































 37%|████████████████████████████████████████████▋                                                                             | 399/1089 [11:49<19:07,  1.66s/it]
 37%|████████████████████████████████████████████▊                                                                             | 400/1089 [11:51<19:10,  1.67s/it][INFO|trainer.py:2926] 2024-01-29 00:50:22,455 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-400
[INFO|tokenization_utils_base.py:2433] 2024-01-29 00:50:22,515 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-29 00:50:22,515 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-400/special_tokens_map.json










































 41%|██████████████████████████████████████████████████▍                                                                       | 450/1089 [13:18<19:07,  1.80s/it]













































 46%|████████████████████████████████████████████████████████                                                                  | 500/1089 [14:48<16:50,  1.72s/it]











































 51%|█████████████████████████████████████████████████████████████▌                                                            | 550/1089 [16:15<14:54,  1.66s/it]












































 55%|███████████████████████████████████████████████████████████████████                                                       | 599/1089 [17:42<14:59,  1.84s/it]
 55%|███████████████████████████████████████████████████████████████████▏                                                      | 600/1089 [17:44<14:37,  1.79s/it][INFO|trainer.py:2926] 2024-01-29 00:56:16,119 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-600
[INFO|tokenization_utils_base.py:2433] 2024-01-29 00:56:16,165 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-29 00:56:16,165 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-600/special_tokens_map.json










































 60%|████████████████████████████████████████████████████████████████████████▋                                                 | 649/1089 [19:11<12:40,  1.73s/it]













































 64%|██████████████████████████████████████████████████████████████████████████████▎                                           | 699/1089 [20:41<11:50,  1.82s/it]












































 69%|███████████████████████████████████████████████████████████████████████████████████▉                                      | 749/1089 [22:10<10:09,  1.79s/it]











































 73%|█████████████████████████████████████████████████████████████████████████████████████████▌                                | 799/1089 [23:36<08:19,  1.72s/it]
 73%|█████████████████████████████████████████████████████████████████████████████████████████▌                                | 800/1089 [23:38<08:39,  1.80s/it][INFO|trainer.py:2926] 2024-01-29 01:02:09,735 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-800
[INFO|tokenization_utils_base.py:2433] 2024-01-29 01:02:09,782 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-29 01:02:09,783 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-800/special_tokens_map.json











































 78%|███████████████████████████████████████████████████████████████████████████████████████████████                           | 849/1089 [25:05<06:54,  1.73s/it]













































 83%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                     | 900/1089 [26:36<05:46,  1.83s/it]













































 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 950/1089 [28:06<04:20,  1.87s/it]











































 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉          | 999/1089 [29:32<02:38,  1.76s/it]
 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 1000/1089 [29:34<02:37,  1.77s/it][INFO|trainer.py:2926] 2024-01-29 01:08:05,968 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-1000
[INFO|tokenization_utils_base.py:2433] 2024-01-29 01:08:06,013 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-29 01:08:06,013 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tmp-checkpoint-1000/special_tokens_map.json











































 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 1050/1089 [31:03<01:08,  1.74s/it]


































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1089/1089 [32:14<00:00,  1.83s/it][INFO|trainer.py:1962] 2024-01-29 01:10:45,532 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1089/1089 [32:14<00:00,  1.78s/it]
[INFO|trainer.py:2926] 2024-01-29 01:10:45,536 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_mistral
[INFO|tokenization_utils_base.py:2433] 2024-01-29 01:10:45,581 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-29 01:10:45,581 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_mistral/special_tokens_map.json
[INFO|modelcard.py:452] 2024-01-29 01:10:45,708 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
{'train_runtime': 1943.2654, 'train_samples_per_second': 2.243, 'train_steps_per_second': 0.56, 'train_loss': 0.43493520107917333, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.4349
  train_runtime            = 0:32:23.26
  train_samples_per_second =      2.243
  train_steps_per_second   =       0.56
Figure saved: /root/LinChance-Fine-tuning-System/output_models/output_mistral/training_loss.png
01/29/2024 01:10:45 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.