
01/27/2024 17:09:34 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.





























  4%|█████▎                                                                                                               | 49/1089 [01:00<21:14,  1.23s/it]































  9%|██████████▌                                                                                                          | 98/1089 [02:02<19:38,  1.19s/it]





























 14%|███████████████▉                                                                                                    | 150/1089 [03:01<19:49,  1.27s/it]































 18%|█████████████████████▏                                                                                              | 199/1089 [04:02<18:18,  1.23s/it]
 18%|█████████████████████▎                                                                                              | 200/1089 [04:04<19:29,  1.32s/it][WARNING|trainer.py:2369] 2024-01-27 17:13:38,665 >> Checkpoint destination directory /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/checkpoint-200 already exists and is non-empty.Saving will proceed but saved results may be invalid.
[INFO|trainer.py:2926] 2024-01-27 17:13:38,665 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/checkpoint-200
[INFO|tokenization_utils_base.py:2433] 2024-01-27 17:13:38,714 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-27 17:13:38,714 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/checkpoint-200/special_tokens_map.json






























 23%|██████████████████████████▌                                                                                         | 249/1089 [05:07<19:22,  1.38s/it]































 27%|███████████████████████████████▊                                                                                    | 299/1089 [06:09<15:15,  1.16s/it]
































 32%|█████████████████████████████████████▎                                                                              | 350/1089 [07:14<15:10,  1.23s/it]































 37%|██████████████████████████████████████████▌                                                                         | 399/1089 [08:15<12:49,  1.11s/it]
 37%|██████████████████████████████████████████▌                                                                         | 400/1089 [08:16<11:59,  1.04s/it][INFO|trainer.py:2926] 2024-01-27 17:17:51,182 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-400
[INFO|tokenization_utils_base.py:2433] 2024-01-27 17:17:51,240 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-27 17:17:51,241 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-400/special_tokens_map.json






























 41%|███████████████████████████████████████████████▊                                                                    | 449/1089 [09:18<12:45,  1.20s/it]

































 46%|█████████████████████████████████████████████████████▎                                                              | 500/1089 [10:25<11:12,  1.14s/it]






























 51%|██████████████████████████████████████████████████████████▋                                                         | 551/1089 [11:26<09:57,  1.11s/it]






























 55%|███████████████████████████████████████████████████████████████▉                                                    | 600/1089 [12:29<10:22,  1.27s/it][INFO|trainer.py:2926] 2024-01-27 17:22:03,449 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-600
[INFO|tokenization_utils_base.py:2433] 2024-01-27 17:22:03,508 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-27 17:22:03,508 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-600/special_tokens_map.json
{'loss': 0.6719, 'learning_rate': 2.1799421557289286e-05, 'epoch': 1.65}





























 60%|█████████████████████████████████████████████████████████████████████▏                                              | 650/1089 [13:28<08:20,  1.14s/it]































 64%|██████████████████████████████████████████████████████████████████████████▍                                         | 699/1089 [14:30<08:09,  1.26s/it]































 69%|███████████████████████████████████████████████████████████████████████████████▉                                    | 750/1089 [15:32<07:10,  1.27s/it]



























 73%|█████████████████████████████████████████████████████████████████████████████████████                               | 798/1089 [16:26<06:03,  1.25s/it]
 73%|█████████████████████████████████████████████████████████████████████████████████████▏                              | 800/1089 [16:29<06:36,  1.37s/it][INFO|trainer.py:2926] 2024-01-27 17:26:04,136 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-800
[INFO|tokenization_utils_base.py:2433] 2024-01-27 17:26:04,194 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-27 17:26:04,194 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-800/special_tokens_map.json






























 78%|██████████████████████████████████████████████████████████████████████████████████████████▍                         | 849/1089 [17:31<04:59,  1.25s/it]
































 83%|███████████████████████████████████████████████████████████████████████████████████████████████▊                    | 899/1089 [18:35<04:06,  1.30s/it]

































 87%|███████████████████████████████████████████████████████████████████████████████████████████████▊              | 949/1089 [1:14:52<18:59:13, 488.24s/it]































 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 1000/1089 [1:15:57<01:44,  1.17s/it][INFO|trainer.py:2926] 2024-01-27 18:25:31,981 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-1000
[INFO|tokenization_utils_base.py:2433] 2024-01-27 18:25:32,039 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-27 18:25:32,040 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-1000/special_tokens_map.json
{'loss': 0.616, 'learning_rate': 1.0330942923610698e-06, 'epoch': 2.75}






























 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 1049/1089 [1:16:59<00:44,  1.12s/it]
























100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1089/1089 [1:17:49<00:00,  1.23s/it][INFO|trainer.py:1962] 2024-01-27 18:27:24,343 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1089/1089 [1:17:49<00:00,  4.29s/it]
[INFO|trainer.py:2926] 2024-01-27 18:27:24,348 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1
[INFO|tokenization_utils_base.py:2433] 2024-01-27 18:27:24,404 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-27 18:27:24,404 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/special_tokens_map.json
[INFO|modelcard.py:452] 2024-01-27 18:27:24,569 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
{'train_runtime': 4678.7275, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.233, 'train_loss': 0.6929886102457458, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  train_loss               =      0.693
  train_runtime            = 1:17:58.72
  train_samples_per_second =      0.932
  train_steps_per_second   =      0.233
Figure saved: /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/training_loss.png
01/27/2024 18:27:24 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.