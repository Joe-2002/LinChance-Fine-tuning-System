






























  5%|█████▎                                                                                                               | 50/1089 [01:01<21:46,  1.26s/it]































  9%|██████████▋                                                                                                         | 100/1089 [02:04<18:04,  1.10s/it]





























 14%|████████████████                                                                                                    | 151/1089 [03:02<17:14,  1.10s/it]






























 18%|█████████████████████▎                                                                                              | 200/1089 [04:04<19:29,  1.32s/it][INFO|trainer.py:2926] 2024-01-27 15:51:16,049 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-200
[INFO|tokenization_utils_base.py:2433] 2024-01-27 15:51:16,114 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-27 15:51:16,115 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/test1/tmp-checkpoint-200/special_tokens_map.json
{'loss': 0.7861, 'learning_rate': 4.6375134823700505e-05, 'epoch': 0.55}































 23%|██████████████████████████▋                                                                                         | 250/1089 [05:08<16:10,  1.16s/it]





















