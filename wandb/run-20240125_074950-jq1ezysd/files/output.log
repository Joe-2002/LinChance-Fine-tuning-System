






























  5%|████▋                                                                                              | 50/1070 [01:01<19:43,  1.16s/it]






























  9%|█████████▏                                                                                         | 99/1070 [02:01<19:34,  1.21s/it]






























 14%|█████████████▋                                                                                    | 150/1070 [03:02<16:56,  1.10s/it]






























 19%|██████████████████▎                                                                               | 200/1070 [04:04<17:49,  1.23s/it][INFO|trainer.py:2926] 2024-01-25 07:53:57,840 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-200
[INFO|tokenization_utils_base.py:2433] 2024-01-25 07:53:57,911 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-25 07:53:57,911 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-200/special_tokens_map.json
{'loss': 0.7366, 'learning_rate': 4.6248606537909465e-05, 'epoch': 0.19}






























 23%|██████████████████████▉                                                                           | 250/1070 [05:06<16:54,  1.24s/it]

































 28%|███████████████████████████▍                                                                      | 300/1070 [06:13<17:09,  1.34s/it]
































 33%|████████████████████████████████                                                                  | 350/1070 [07:17<12:16,  1.02s/it]






























 37%|████████████████████████████████████▍                                                             | 398/1070 [08:17<12:41,  1.13s/it]
 37%|████████████████████████████████████▋                                                             | 400/1070 [08:18<11:12,  1.00s/it][INFO|trainer.py:2926] 2024-01-25 07:58:12,331 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-400
[INFO|tokenization_utils_base.py:2433] 2024-01-25 07:58:12,388 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-25 07:58:12,389 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-400/special_tokens_map.json






























 42%|█████████████████████████████████████████                                                         | 449/1070 [09:20<13:14,  1.28s/it]
































 47%|█████████████████████████████████████████████▋                                                    | 499/1070 [10:24<12:19,  1.29s/it]































 51%|██████████████████████████████████████████████████▎                                               | 549/1070 [11:26<11:30,  1.33s/it]































 56%|██████████████████████████████████████████████████████▊                                           | 598/1070 [12:27<12:04,  1.54s/it]
 56%|██████████████████████████████████████████████████████▉                                           | 600/1070 [12:29<10:17,  1.31s/it][INFO|trainer.py:2926] 2024-01-25 08:02:23,314 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-600
[INFO|tokenization_utils_base.py:2433] 2024-01-25 08:02:23,368 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-25 08:02:23,369 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-600/special_tokens_map.json






























 61%|███████████████████████████████████████████████████████████▍                                      | 649/1070 [13:32<08:50,  1.26s/it]































 65%|████████████████████████████████████████████████████████████████                                  | 699/1070 [14:34<07:32,  1.22s/it]































 70%|████████████████████████████████████████████████████████████████████▌                             | 749/1070 [15:35<06:05,  1.14s/it]































 75%|█████████████████████████████████████████████████████████████████████████▏                        | 799/1070 [16:38<06:29,  1.44s/it]
 75%|█████████████████████████████████████████████████████████████████████████▎                        | 800/1070 [16:39<06:18,  1.40s/it][INFO|trainer.py:2926] 2024-01-25 08:06:32,909 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-800
[INFO|tokenization_utils_base.py:2433] 2024-01-25 08:06:32,965 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-25 08:06:32,965 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-800/special_tokens_map.json





























 79%|█████████████████████████████████████████████████████████████████████████████▊                    | 849/1070 [17:38<04:19,  1.17s/it]































 84%|██████████████████████████████████████████████████████████████████████████████████▍               | 900/1070 [18:40<03:12,  1.13s/it]






























 89%|██████████████████████████████████████████████████████████████████████████████████████▉           | 949/1070 [19:41<02:18,  1.14s/it]





























 93%|██████████████████████████████████████████████████████████████████████████████████████████▋      | 1000/1070 [20:40<01:29,  1.28s/it][INFO|trainer.py:2926] 2024-01-25 08:10:34,237 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-1000
[INFO|tokenization_utils_base.py:2433] 2024-01-25 08:10:34,301 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-25 08:10:34,302 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tmp-checkpoint-1000/special_tokens_map.json
{'loss': 0.6476, 'learning_rate': 7.036623822514049e-07, 'epoch': 0.93}






























 98%|███████████████████████████████████████████████████████████████████████████████████████████████  | 1049/1070 [21:42<00:24,  1.17s/it]












100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1070/1070 [22:08<00:00,  1.41s/it][INFO|trainer.py:1962] 2024-01-25 08:12:02,331 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1070/1070 [22:08<00:00,  1.24s/it]
[INFO|trainer.py:2926] 2024-01-25 08:12:02,336 >> Saving model checkpoint to /root/LinChance-Fine-tuning-System/output_models/output_baichuan2
[INFO|tokenization_utils_base.py:2433] 2024-01-25 08:12:02,398 >> tokenizer config file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-01-25 08:12:02,398 >> Special tokens file saved in /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/special_tokens_map.json
[INFO|modelcard.py:452] 2024-01-25 08:12:02,567 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
{'train_runtime': 1333.6992, 'train_samples_per_second': 3.211, 'train_steps_per_second': 0.802, 'train_loss': 0.7259625015971817, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =      0.726
  train_runtime            = 0:22:13.69
  train_samples_per_second =      3.211
  train_steps_per_second   =      0.802
Figure saved: /root/LinChance-Fine-tuning-System/output_models/output_baichuan2/training_loss.png
01/25/2024 08:12:02 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.